{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import quote  # 이 부분 추가\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import threading\n",
    "import re  # 코드 상단에 추가\n",
    "\n",
    "# PDF 저장 디렉토리와 로그 디렉토리 설정\n",
    "PDF_DIR = \"data/samsung\"\n",
    "LOG_DIR = \"data/logs\"\n",
    "\n",
    "# 디렉토리 생성\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# 로그 설정\n",
    "log_file = os.path.join(LOG_DIR, f\"news_download_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file, encoding='utf-8'),\n",
    "        # logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebDriver 인스턴스를 관리하는 싱글톤 클래스\n",
    "class ChromeDriverWrapper:\n",
    "    \"\"\"Chrome WebDriver를 관리하는 싱글톤 클래스\"\"\"\n",
    "    _instance = None\n",
    "    _driver = None\n",
    "    _lock = threading.Lock()\n",
    "    \n",
    "    @classmethod\n",
    "    def get_instance(cls):\n",
    "        if cls._instance is None:\n",
    "            with cls._lock:\n",
    "                if cls._instance is None:\n",
    "                    cls._instance = cls()\n",
    "        return cls._instance\n",
    "    \n",
    "    def get_driver(self):\n",
    "        if self._driver is None:\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            \n",
    "            self._driver = webdriver.Chrome(\n",
    "                service=Service(ChromeDriverManager().install()),\n",
    "                options=chrome_options\n",
    "            )\n",
    "        return self._driver\n",
    "    \n",
    "    def quit_driver(self):\n",
    "        if self._driver:\n",
    "            try:\n",
    "                self._driver.quit()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"드라이버 종료 중 오류: {str(e)}\")\n",
    "            finally:\n",
    "                self._driver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... 기존 import 및 환경설정 코드 생략 ...\n",
    "\n",
    "class SamsungSemiconNewsScraper:\n",
    "    def __init__(self):\n",
    "        logging.info(\"스크래퍼 초기화 시작\")\n",
    "        self.categories = [\n",
    "            {\n",
    "                \"name\": \"프레스센터\",\n",
    "                \"url\": \"https://news.samsungsemiconductor.com/kr/category/%eb%89%b4%ec%8a%a4/\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"문화\",\n",
    "                \"url\": \"https://news.samsungsemiconductor.com/kr/category/%eb%ac%b8%ed%99%94/\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ESG\",\n",
    "                \"url\": \"https://news.samsungsemiconductor.com/kr/category/esg/\"\n",
    "            }\n",
    "        ]\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "        self.driver = webdriver.Chrome(\n",
    "            service=Service(ChromeDriverManager().install()),\n",
    "            options=chrome_options\n",
    "        )\n",
    "        self.wait = WebDriverWait(self.driver, 20)\n",
    "        self.articles = []\n",
    "        logging.info(\"스크래퍼 초기화 완료\")\n",
    "\n",
    "    def get_article_content(self, url):\n",
    "        logging.info(f\"기사 내용 추출 시작: {url}\")\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(2)\n",
    "            content_container = self.wait.until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.content_view > div.content_desc\"))\n",
    "            )\n",
    "            paragraphs = content_container.find_elements(By.TAG_NAME, \"p\")\n",
    "            content_texts = [p.text.strip() for p in paragraphs if p.text.strip()]\n",
    "            content = '\\n'.join(content_texts)\n",
    "            return content if content else \"내용 추출 실패\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"본문 추출 중 오류: {str(e)}\")\n",
    "            return \"내용 추출 실패\"\n",
    "\n",
    "    def collect_articles_from_category(self, category_name, category_url, page_limit=None):\n",
    "        total_collected_pages = 0\n",
    "        current_page = 1\n",
    "\n",
    "        # page_limit이 설정된 경우 목표 페이지 수를 로깅\n",
    "        if page_limit:\n",
    "            logging.info(f\"[{category_name}] 목표 페이지 수: {page_limit}\")\n",
    "\n",
    "        while True:\n",
    "            # 현재 페이지 URL 구성\n",
    "            if current_page == 1:\n",
    "                page_url = category_url\n",
    "            else:\n",
    "                page_url = f\"{category_url}page/{current_page}/\"\n",
    "            \n",
    "            logging.info(f\"[{category_name}] {current_page}페이지 접속: {page_url}\")\n",
    "            self.driver.get(page_url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            # 404 페이지 체크\n",
    "            try:\n",
    "                error_page = self.driver.find_element(By.CSS_SELECTOR, \"div.page_404\")\n",
    "                if error_page:\n",
    "                    logging.info(f\"[{category_name}] {current_page}페이지에서 404 에러 발생. 수집을 종료합니다.\")\n",
    "                    break\n",
    "            except:\n",
    "                pass  # 404 페이지가 아닌 경우 정상 진행\n",
    "\n",
    "            # 기사 수집\n",
    "            try:\n",
    "                articles = self.driver.find_elements(By.CSS_SELECTOR, \"ul.article_list > li.article_item\")\n",
    "                if not articles:\n",
    "                    logging.info(f\"[{category_name}] {current_page}페이지에 기사가 없습니다.\")\n",
    "                    break\n",
    "                \n",
    "                logging.info(f\"[{category_name}] {current_page}페이지 기사 수: {len(articles)}\")\n",
    "                article_infos = []\n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        link_element = article.find_element(By.TAG_NAME, \"a\")\n",
    "                        article_url = link_element.get_attribute(\"href\")\n",
    "                        title = article.find_element(By.CSS_SELECTOR, \"p.title\").text.strip()\n",
    "                        date = article.find_element(By.CSS_SELECTOR, \"span.date\").text.strip()\n",
    "                        try:\n",
    "                            category = article.find_element(By.CSS_SELECTOR, \"span.category\").text.strip()\n",
    "                        except:\n",
    "                            category = \"\"\n",
    "                        try:\n",
    "                            desc = article.find_element(By.CSS_SELECTOR, \"p.desc\").text.strip()\n",
    "                        except:\n",
    "                            desc = \"\"\n",
    "                        article_infos.append({\n",
    "                            \"category_group\": category_name,\n",
    "                            \"title\": title,\n",
    "                            \"url\": article_url,\n",
    "                            \"date\": date,\n",
    "                            \"category\": category,\n",
    "                            \"desc\": desc\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"기사 리스트 정보 추출 중 오류: {str(e)}\")\n",
    "\n",
    "                for info in article_infos:\n",
    "                    content = self.get_article_content(info[\"url\"])\n",
    "                    info[\"content\"] = content\n",
    "                    self.articles.append(info)\n",
    "                    logging.info(f\"기사 추가: {info['title']}\")\n",
    "\n",
    "                total_collected_pages += 1\n",
    "\n",
    "                # 페이지 제한 체크\n",
    "                if page_limit:\n",
    "                    if current_page >= page_limit:\n",
    "                        logging.info(f\"[{category_name}] 목표 페이지 수({page_limit})에 도달하여 수집을 종료합니다.\")\n",
    "                        break\n",
    "                    else:\n",
    "                        logging.info(f\"[{category_name}] {current_page}/{page_limit} 페이지 수집 완료\")\n",
    "                else:\n",
    "                    logging.info(f\"[{category_name}] {current_page}페이지 수집 완료\")\n",
    "\n",
    "                current_page += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"페이지 {current_page} 처리 중 오류: {str(e)}\")\n",
    "                break\n",
    "\n",
    "        logging.info(f\"[{category_name}] 기사 수집 완료 (총 {total_collected_pages}페이지)\")\n",
    "\n",
    "    def search_news(self, page_limit=None):\n",
    "        for cat in self.categories:\n",
    "            logging.info(f\"{cat['name']} 기사 수집 시작\")\n",
    "            self.collect_articles_from_category(cat[\"name\"], cat[\"url\"], page_limit=page_limit)\n",
    "            logging.info(f\"{cat['name']} 기사 수집 완료\")\n",
    "\n",
    "    def save_to_csv(self, filename):\n",
    "        logging.info(f\"CSV 파일 저장 시작: {filename}\")\n",
    "        try:\n",
    "            if not self.articles:\n",
    "                logging.warning(\"저장할 기사가 없습니다\")\n",
    "                return\n",
    "            df = pd.DataFrame(self.articles)\n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            logging.info(\"CSV 파일 저장 완료\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"CSV 파일 저장 중 오류: {str(e)}\")\n",
    "\n",
    "    def __del__(self):\n",
    "        try:\n",
    "            self.driver.quit()\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scraper = SamsungSemiconNewsScraper()\n",
    "    scraper.search_news(page_limit=85)  # 또는 scraper.search_news()\n",
    "    scraper.save_to_csv(\"data/samsung/leadership_news.csv\")\n",
    "finally:\n",
    "    scraper.__del__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_scraping",
   "language": "python",
   "name": "news_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
